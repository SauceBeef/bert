# colab版本


# 静态mask 动态权重
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from torch.utils.data import DataLoader
import torch
from math import sqrt
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
import random


def get_txt(data_path):
    book = xlrd.open_workbook(data_path)
    sheet = book.sheet_by_index(0)
    # print(sheet.cell_value(rowx=1, colx=1))
    file = open('description.txt', mode='w', encoding='utf-8')
    file.write('')  # 清除原内容
    file.close()
    # 读入description
    file = open('description.txt', mode='a', encoding='utf-8')
    for i in range(sheet.nrows - 2):
        if sheet.cell_value(rowx=i + 2, colx=20) != 0:
            file.write('1 ' + sheet.cell_value(rowx=i + 2, colx=16) + '\n')
        else:
            file.write('0 ' + sheet.cell_value(rowx=i + 2, colx=16) + '\n')
    file.close()


# 屏蔽字符
def split_txt_punc(raw_texts):
    texts = []
    replace_list = ["(",")","\"","“","”","’","\'",",",":","-"]
    for raw_text in raw_texts:
        for symbol in replace_list:
          raw_text = raw_text.replace(symbol, "")
        texts.append(raw_text[0:-1])
    return texts


# 获取文本和标记
def split_txt_label(data_path):
    file = open(data_path, mode='r', encoding='utf-8')
    texts = []
    labels = []
    raw_texts = file.readlines()
    for raw_text in raw_texts:
        texts.append(raw_text[1:-1])
        labels.append(raw_text[0])
    file.close()
    return texts, labels


def delete_tail_split(data_path):
    file = open(data_path, mode='r', encoding='utf-8')
    texts = []
    labels = []
    raw_texts = file.readlines()
    for raw_text in raw_texts:
        texts.append(raw_text[1:-1])
        labels.append(raw_text[0])
    file.close()
    print(texts)
    new_texts = []
    for text in texts:
        new_text = text.split(',')[0:-3]
        temp = ""
        for i in new_text:
            temp += i
        new_texts.append(temp)
    return new_texts, labels


# 计算 不同类别acc
def accuracy_calculate(predicted_list, true_list):
    true_preds = [0, 0]
    class_sum = [0, 0]
    final_acc = [0.0, 0.0]
    for i in range(len(true_list)):
        if true_list[i] == 0:
            class_sum[0] += 1
        else:
            class_sum[1] += 1
    for i in range(len(predicted_list)):
        if predicted_list[i] == true_list[i]:
            if predicted_list[i] == 0:
                true_preds[0] += 1
            else:
                true_preds[1] += 1
    for cls in range(len(class_sum)):
        if class_sum[cls] == 0:
            final_acc[cls] = 0.0
        else:
            final_acc[cls] = true_preds[cls] / class_sum[cls]
    return final_acc


def get_F1(predicted_labels, true_labels):
    true_positive = 0
    positive_labels = 0
    positive_preds = 0
    for i in range(len(predicted_labels)):
      if predicted_labels[i] == 1:
        positive_preds += 1
        if predicted_labels[i] == true_labels[i]:
          true_positive += 1
      if true_labels[i] == 1:
        positive_labels += 1
    if positive_preds == 0:
      return 0
    precision = true_positive / positive_preds
    recall = true_positive / positive_labels
    F1 = 2 * precision * recall / (precision + recall)
    return F1

def get_MCC(predicted_labels, true_labels):
    TP, FP, TN, FN = (0, 0, 0, 0)
    for i in range(len(predicted_labels)):
      if predicted_labels[i] == 1:
        if predicted_labels[i] == true_labels[i]:
          TP += 1
        else:
          FP += 1
      else:
        if predicted_labels[i] == true_labels[i]:
          TN += 1
        else:
          FN += 1
    if (TP+FP)*(FN+TP)*(FN+TN)*(FP+TN) == 0:
      return 0
    MCC = (TP*TN - FP*FN) / sqrt((TP+FP)*(FN+TP)*(FN+TN)*(FP+TN))
    return MCC



def get_new_dict(path):
  texts, labels = split_txt_label(path)
  unk_dict = set()
  tokenizer = BertTokenizer.from_pretrained('./MyTokenizer')
  for text in texts:
    tokens = tokenizer.basic_tokenizer.tokenize(text)
    input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens))
    if len(tokens) < 256:
        input_ids = torch.cat([input_ids, torch.zeros(256-len(tokens))]).resize(1, 256)
    else:
        input_ids = input_ids.resize(1, len(tokens))
    for i in range(256):
        if input_ids[0][i] == 100:
            unk_dict.add(tokens[i])
  return list(unk_dict)


def random_mask(input_ids, attention_mask, mask_amount):
    mask = 0
    # 将有内容的部分随机选取mask_amount个id进行屏蔽
    # mask_amount为0则按比例选取
    if mask_amount==0:
      mask_amount = int(len(input_ids) * 0.15)
    for i in range(mask_amount):
        while (input_ids[0][mask] == 0) | (mask == 0):
            mask = random.sample(range(len(input_ids[0])), 1)
            break
        input_ids[0][mask] = 0
        attention_mask[0][mask] = 0
    return input_ids, attention_mask


def dataset(data_path, random_state, test_size=0.15, mask_amount=3):
    texts, labels = split_txt_label(data_path)
    texts = split_txt_punc(texts)
    labels = [0 if label == '0' else 1 for label in labels]
    input_ids_list = []
    attention_mask_list = []
    tokenizer = BertTokenizer.from_pretrained('./MyTokenizer')
    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=256,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        # 对injured、uninjured类同时随机屏蔽mask_amount个id
        mask_input_ids, mask_attention_mask = random_mask(encoded['input_ids'], encoded['attention_mask'],
                                                          mask_amount=mask_amount)
        input_ids_list.append(mask_input_ids)
        attention_mask_list.append(mask_attention_mask)
    input_ids = torch.cat(input_ids_list, dim=0)
    attention_mask = torch.cat(attention_mask_list, dim=0)
    labels = torch.tensor(labels)
    train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels = \
        train_test_split(input_ids, attention_mask, labels, random_state=random_state, test_size=test_size,
                         shuffle=True)
    # 打乱不同年份的数据
    train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_labels)
    test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_attention_mask, test_labels)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)

    return train_loader, test_loader


def dataloader(data_path, turns, test_size, mask_amount):
    train_loaders = []
    test_loaders = []
    for turn in range(turns):
        train_loader, test_loader = dataset(data_path, turn, test_size=test_size, mask_amount=mask_amount)
        train_loaders.append(train_loader)
        test_loaders.append(test_loader)

    return train_loaders, test_loaders


def n_fold_cross_validation_train(turns, epochs, path, test_size, mask_amount):
    train_loaders, test_loaders = dataloader(path, turns, test_size, mask_amount)
    acc_list_train = []
    acc_list_test = []
    acc_list_0 = []
    acc_list_1 = []
    F1_list = []
    auc_list = []
    mcc_list = []
    unk_list = get_new_dict(path)
    tokenizer = BertTokenizer.from_pretrained('./MyTokenizer')
    tokenizer.add_tokens(unk_list)
    for turn in range(turns):
        train_loader = train_loaders[turn]
        test_loader = test_loaders[turn]
        # 使用经过训练的初始化
        model = BertForSequenceClassification.from_pretrained('./MyBert_pos50_8layers')
        #
        model.resize_token_embeddings(len(tokenizer))
        # 5. 定义优化器和损失函数
        optimizer = torch.optim.AdamW(model.parameters(), lr=1.35e-5)
        criterion = torch.nn.CrossEntropyLoss()
        # 6. 训练模型
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        torch.cuda.empty_cache()
        model.to(device)
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0
            train_acc = 0.0
            for input_ids, attention_mask, labels in train_loader:
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)
                class_count = [0, 0]
                class_weight = [0.0, 0.0]
                for label in labels:
                    if label == '0':
                        class_count[0] += 1
                    else:
                        class_count[1] += 1
                # 以train_loader为单位进行计算权重
                if (class_count[0] == 0) | (class_count[1] == 0):
                    class_weight = torch.tensor([1, 1], dtype=torch.float32)
                else:
                    class_weight = torch.tensor([1 / class_num for class_num in class_count], dtype=torch.float32)
                class_weight[1] *= 50
                labels = labels.to(device)
                # class_weight = torch.tensor([1 / 351, 1 / 62])
                # [1/351, 1/62]为样本总权重
                class_weight_tensor = class_weight.to(device)
                loss_fct = nn.CrossEntropyLoss(weight=class_weight_tensor)
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                logits = outputs.logits
                logits = logits.to(device)
                loss = loss_fct(outputs.logits, labels)
                train_loss += loss.item()
                _, preds = torch.max(logits, dim=1)
                train_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy())
                loss.backward()
                optimizer.step()
            train_loss /= len(train_loader)
            train_acc /= len(train_loader)
            acc_list_train.append(train_acc)
            print(f"Epoch {epoch + 1}/{epochs}")
            print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}")
            if train_loss < 0.005:
              print('training stop')
              break
            if train_acc > 0.995:
              print('training stop')
              break
        print(f'-----------trainload:{turn + 1}------------')
        model.eval()
        predicted_labels = []
        true_labels = []
        with torch.no_grad():
            for input_ids, attention_mask, labels in test_loader:
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)
                labels = labels.to(device)
                outputs = model(input_ids, attention_mask=attention_mask)
                _, preds = torch.max(outputs.logits, dim=1)
                predicted_labels.extend(preds.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        predicted_labels = np.array(predicted_labels)
        true_labels = np.array(true_labels)
        # 记录 fpr,tpr 计算auc
        fpr, tpr, thresholds = roc_curve(true_labels, predicted_labels, pos_label=1)
        auc_list.append(auc(fpr, tpr))
        accuracy = accuracy_score(true_labels, predicted_labels)
        acc_list_test.append(accuracy)
        if get_MCC(predicted_labels, true_labels)!=0:
            mcc_list.append(get_MCC(predicted_labels, true_labels))
        final_acc = accuracy_calculate(predicted_list=predicted_labels, true_list=true_labels)
        if final_acc[0] != 0:
            acc_list_0.append(final_acc[0])
        if final_acc[1] != 0:
            acc_list_1.append(final_acc[1])
        F1 = get_F1(predicted_labels, true_labels)
        if F1!=0:
            F1_list.append(F1)
        print("Test Accuracy: {:.2%}".format(accuracy))
        print(f'----------------------------------')
        
    final_train_acc = sum(acc_list_train) / len(acc_list_train)
    final_test_acc = sum(acc_list_test) / len(acc_list_test)
    final_acc_label_0 = sum(acc_list_0) / len(acc_list_0)
    final_acc_label_1 = sum(acc_list_1) / len(acc_list_1)
    final_F1 = sum(F1_list) / len(F1_list)
    final_auc = sum(auc_list) / len(auc_list)
    final_mcc = sum(mcc_list) / len(mcc_list)
    print(f'final_train_acc:{final_train_acc}')
    print(f'final_test_acc:{final_test_acc}')
    print(f'final_specificity:{final_acc_label_0}')
    print(f'final_sensitivity:{final_acc_label_1}')
    print(f'final_F1:{final_F1}')
    print(f'final_auc:{final_auc}')
    print(f'final_mcc:{final_mcc}')
    print(f'final_G-mean:{sqrt(final_acc_label_0 * final_acc_label_1)}')


%env CUDA_LAUNCH_BLOCKING=1
torch.cuda.empty_cache()
n_fold_cross_validation_train(40, 45, './description_save.txt',
                              test_size=0.2, mask_amount=0)
